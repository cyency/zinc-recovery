{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Concatenate, Add, Activation, Multiply, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.losses import mean_squared_error, mean_absolute_error\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training/test data - adjust for local system\n",
    "SRC_DIR = \"../data/\"\n",
    "\n",
    "allDataFile = SRC_DIR + \"/all_data_cleaned.csv\"\n",
    "haveAllData = os.path.isfile(allDataFile)\n",
    "\n",
    "if haveAllData:\n",
    "    allData = pd.read_csv(allDataFile)\n",
    "    allData = allData[ allData.columns[1:]]\n",
    "    # Move December to testing files\n",
    "    t00 = datetime.datetime(2016, 12, 1)\n",
    "    t01 = datetime.datetime(2017, 1, 1)\n",
    "    t10 = datetime.datetime(2017, 12, 1)\n",
    "    t11 = datetime.datetime(2018, 1, 1)\n",
    "    dates = pd.to_datetime( allData['date'] )\n",
    "    testDates =  ( (dates>=t00) & (dates<t01) ) | (  (dates>=t10) & (dates<t11) ).values\n",
    "    \n",
    "    testColumns = ['date', 'primary_cleaner.input.copper_sulfate',\n",
    "       'primary_cleaner.input.depressant', 'primary_cleaner.input.feed_size',\n",
    "       'primary_cleaner.input.xanthate','primary_cleaner.state.floatbank8_a_air',\n",
    "       'primary_cleaner.state.floatbank8_a_level',\n",
    "       'primary_cleaner.state.floatbank8_b_air',\n",
    "       'primary_cleaner.state.floatbank8_b_level',\n",
    "       'primary_cleaner.state.floatbank8_c_air',\n",
    "       'primary_cleaner.state.floatbank8_c_level',\n",
    "       'primary_cleaner.state.floatbank8_d_air',\n",
    "       'primary_cleaner.state.floatbank8_d_level', 'rougher.input.feed_fe',\n",
    "       'rougher.input.feed_pb', 'rougher.input.feed_rate',\n",
    "       'rougher.input.feed_size', 'rougher.input.feed_sol',\n",
    "       'rougher.input.feed_zn', 'rougher.input.floatbank10_copper_sulfate',\n",
    "       'rougher.input.floatbank10_xanthate',\n",
    "       'rougher.input.floatbank11_copper_sulfate',\n",
    "       'rougher.input.floatbank11_xanthate', 'rougher.state.floatbank10_a_air',\n",
    "       'rougher.state.floatbank10_a_level', 'rougher.state.floatbank10_b_air',\n",
    "       'rougher.state.floatbank10_b_level', 'rougher.state.floatbank10_c_air',\n",
    "       'rougher.state.floatbank10_c_level', 'rougher.state.floatbank10_d_air',\n",
    "       'rougher.state.floatbank10_d_level', 'rougher.state.floatbank10_e_air',\n",
    "       'rougher.state.floatbank10_e_level', 'rougher.state.floatbank10_f_air',\n",
    "       'rougher.state.floatbank10_f_level',\n",
    "       'secondary_cleaner.state.floatbank2_a_air',\n",
    "       'secondary_cleaner.state.floatbank2_a_level',\n",
    "       'secondary_cleaner.state.floatbank2_b_air',\n",
    "       'secondary_cleaner.state.floatbank2_b_level',\n",
    "       'secondary_cleaner.state.floatbank3_a_air',\n",
    "       'secondary_cleaner.state.floatbank3_a_level',\n",
    "       'secondary_cleaner.state.floatbank3_b_air',\n",
    "       'secondary_cleaner.state.floatbank3_b_level',\n",
    "       'secondary_cleaner.state.floatbank4_a_air',\n",
    "       'secondary_cleaner.state.floatbank4_a_level',\n",
    "       'secondary_cleaner.state.floatbank4_b_air',\n",
    "       'secondary_cleaner.state.floatbank4_b_level',\n",
    "       'secondary_cleaner.state.floatbank5_a_air',\n",
    "       'secondary_cleaner.state.floatbank5_a_level',\n",
    "       'secondary_cleaner.state.floatbank5_b_air',\n",
    "       'secondary_cleaner.state.floatbank5_b_level',\n",
    "       'secondary_cleaner.state.floatbank6_a_air',\n",
    "       'secondary_cleaner.state.floatbank6_a_level']\n",
    "    \n",
    "    trainingData = allData[ testDates==False ]\n",
    "    testData = allData[ testDates ]\n",
    "    \n",
    "    trainingFile = SRC_DIR + \"/gen_all_train.csv\"\n",
    "    trainingData.to_csv(trainingFile, index=False)\n",
    "    testingFile = SRC_DIR + \"/gen_all_test.csv\"\n",
    "    testData[testColumns].to_csv(testingFile, index=False)\n",
    "    \n",
    "    truthData = testData[['date', 'rougher.output.recovery','final.output.recovery']]\n",
    "    compareFile = SRC_DIR + \"/gen_truth.csv\"\n",
    "    truthData.to_csv(compareFile, index=False)\n",
    "    \n",
    "\n",
    "else:\n",
    "    \n",
    "    trainingFile = SRC_DIR + \"/all_train.csv\"\n",
    "    testingFile = SRC_DIR + \"/all_test.csv\"\n",
    "\n",
    "    # Optional - compare against submitted entry\n",
    "    compareFile = \"../submissions/base.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fixFeatures(frame, fixMode):\n",
    "    \n",
    "    combine = [\n",
    "     [ 'primary_cleaner.state.floatbank8_a_air',\n",
    "       'primary_cleaner.state.floatbank8_b_air',\n",
    "       'primary_cleaner.state.floatbank8_c_air',\n",
    "       'primary_cleaner.state.floatbank8_d_air'],\n",
    "        \n",
    "     [ 'primary_cleaner.state.floatbank8_c_level',\n",
    "       'primary_cleaner.state.floatbank8_a_level',\n",
    "       'primary_cleaner.state.floatbank8_b_level',\n",
    "       'primary_cleaner.state.floatbank8_d_level' ], \n",
    "           \n",
    "               \n",
    "     [  'rougher.state.floatbank10_a_air',\n",
    "        'rougher.state.floatbank10_b_air',\n",
    "        'rougher.state.floatbank10_c_air',\n",
    "        'rougher.state.floatbank10_d_air',\n",
    "        'rougher.state.floatbank10_e_air',\n",
    "        'rougher.state.floatbank10_f_air', ],\n",
    "        \n",
    "     [ 'rougher.state.floatbank10_a_level',\n",
    "       'rougher.state.floatbank10_b_level', \n",
    "       'rougher.state.floatbank10_c_level', \n",
    "       'rougher.state.floatbank10_d_level', \n",
    "       'rougher.state.floatbank10_e_level', \n",
    "       'rougher.state.floatbank10_f_level', ],\n",
    "               \n",
    "      ['secondary_cleaner.state.floatbank2_a_air',\n",
    "       'secondary_cleaner.state.floatbank2_b_air',\n",
    "       'secondary_cleaner.state.floatbank3_a_air',\n",
    "       'secondary_cleaner.state.floatbank3_b_air',\n",
    "       'secondary_cleaner.state.floatbank4_a_air',\n",
    "       'secondary_cleaner.state.floatbank4_b_air',\n",
    "       'secondary_cleaner.state.floatbank5_a_air',\n",
    "       'secondary_cleaner.state.floatbank5_b_air',\n",
    "       'secondary_cleaner.state.floatbank6_a_air',],\n",
    "        \n",
    "       ['secondary_cleaner.state.floatbank2_a_level',\n",
    "       'secondary_cleaner.state.floatbank2_b_level',\n",
    "       'secondary_cleaner.state.floatbank3_a_level',\n",
    "       'secondary_cleaner.state.floatbank3_b_level',\n",
    "       'secondary_cleaner.state.floatbank4_a_level',\n",
    "       'secondary_cleaner.state.floatbank4_b_level',\n",
    "       'secondary_cleaner.state.floatbank5_a_level',\n",
    "       'secondary_cleaner.state.floatbank5_b_level',\n",
    "       'secondary_cleaner.state.floatbank6_a_level',],        \n",
    "    ]\n",
    "       \n",
    "    # Replace separate channels with aggregates\n",
    "    \n",
    "    if fixMode!='NoFix':\n",
    "        filled = frame.fillna(method='ffill')\n",
    "        for i,c in enumerate(combine):                 \n",
    "            frame['combine' + str(i)] = filled[c].values.mean(axis=1)       \n",
    "            if (fixMode=='FixReplace'):\n",
    "                frame.drop(columns=c, inplace=True)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixMode = 'NoFix'\n",
    "fixMode = 'FixReplace'\n",
    "#fixMode = 'Fix'\n",
    "\n",
    "\n",
    "frame = fixFeatures(pd.read_csv(trainingFile), fixMode)\n",
    "#print(frame.columns)\n",
    "\n",
    "test = pd.read_csv(testingFile)\n",
    "testDate = test['date']\n",
    "test = fixFeatures(test, fixMode)\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allOutputs = [\n",
    "    'rougher.output.recovery',\n",
    "    'final.output.recovery',\n",
    "    \n",
    "    'rougher.output.concentrate_fe',\n",
    "    'rougher.output.concentrate_pb',\n",
    "    'rougher.output.concentrate_sol',\n",
    "    'rougher.output.concentrate_zn',                                                      \n",
    "    'rougher.output.tail_fe',\n",
    "    'rougher.output.tail_pb',\n",
    "    'rougher.output.tail_sol',\n",
    "    'rougher.output.tail_zn',\n",
    "\n",
    "    'primary_cleaner.output.concentrate_fe',\n",
    "    'primary_cleaner.output.concentrate_pb',\n",
    "    'primary_cleaner.output.concentrate_sol',\n",
    "    'primary_cleaner.output.concentrate_zn',\n",
    "    \n",
    "    'primary_cleaner.output.tail_fe', \n",
    "    'primary_cleaner.output.tail_pb',\n",
    "    'primary_cleaner.output.tail_sol',\n",
    "    'primary_cleaner.output.tail_zn',\n",
    "\n",
    "    'final.output.concentrate_fe', \n",
    "    'final.output.concentrate_pb',\n",
    "    'final.output.concentrate_sol',\n",
    "    'final.output.concentrate_zn',\n",
    "    \n",
    "    'secondary_cleaner.output.tail_fe',\n",
    "    'secondary_cleaner.output.tail_pb',\n",
    "    'secondary_cleaner.output.tail_sol',\n",
    "    'secondary_cleaner.output.tail_zn',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip date\n",
    "inVals = frame[test.columns[1:]]\n",
    "insValid = inVals.notna().all(axis=1).values\n",
    "# If any of the main variables is bad, then don't use for training...\n",
    "znValid = (inVals['rougher.input.feed_zn'].notna().values *\n",
    "           inVals['rougher.input.feed_size'].notna().values *\n",
    "           inVals['rougher.input.feed_rate'].notna().values\n",
    "          )\n",
    "#How much is left?\n",
    "print(\"Zn valid:\", znValid.mean())\n",
    "\n",
    "#Forward fill to preserve causality \n",
    "inVals = inVals.fillna(method='ffill').values\n",
    "allOutVals = frame[allOutputs]\n",
    "outsValid = allOutVals.notna().all(axis=1).values\n",
    "allOutVals = allOutVals.fillna(method='ffill').values\n",
    "\n",
    "\n",
    "inMean = inVals.mean(axis=0)\n",
    "inStd = inVals.std(axis=0)\n",
    "\n",
    "allOutStd = allOutVals.std(axis=0)\n",
    "allOutMean = allOutVals.mean(axis=0)\n",
    "\n",
    "\n",
    "outMean = allOutMean[0:2]\n",
    "outStd = allOutStd[0:2]\n",
    "\n",
    "# Normalized\n",
    "vals_x = (inVals-inMean)/inStd\n",
    "vals_y = (allOutVals-allOutMean)/allOutStd\n",
    "\n",
    "valid = insValid * outsValid\n",
    "\n",
    "print(\"Valid\", valid.mean(), \"in:\", insValid.mean(), \"outs:\", outsValid.mean())\n",
    "print(\"Out Scale\", outStd[0:2])\n",
    "\n",
    "testInsValid = test.notna().all(axis=1).values\n",
    "print('Test inputs valid', testInsValid.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show0 = 1000\n",
    "show1 = 2000\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.gca().set_title(\"Input samples\")\n",
    "for col in range(vals_x.shape[1]):\n",
    "    plt.plot(vals_x[show0:show1,col])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.gca().set_title(\"Output samples\")\n",
    "for col in range(vals_y.shape[1]):\n",
    "    plt.plot(vals_y[show0:show1,col])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.gca().set_title(\"Target samples\")\n",
    "for col in range(2):\n",
    "    plt.plot(vals_y[show0:show1,col])\n",
    "plt.plot(outsValid[show0:show1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time steps to use\n",
    "T = 6\n",
    "\n",
    "# Split between traning and validation set.\n",
    "#\n",
    "# Use large splitTrain to train on all the data\n",
    "\n",
    "# Hold back a test set (False = for developement) or use all data (True = for submission)\n",
    "# I'm not actually sure if the submission had this set to True or False, but True should get better results on average.\n",
    "trainOnAllData = haveAllData\n",
    "\n",
    "testMode = \"TNo\" if trainOnAllData else \"TYes\"\n",
    "\n",
    "\n",
    "#dataMode = \"ValNone\"\n",
    "dataMode = \"ValHf\"\n",
    "#dataMode = \"ValLf\"\n",
    "\n",
    "\n",
    "if dataMode==\"ValNone\":\n",
    "    # Don't need a validation set here, since we are using a fixed training schedule, so use all for training\n",
    "    #  by making the split bigger than the total data\n",
    "    splitTrain = 30000\n",
    "    splitValid = 4\n",
    "    allowOverlap = False\n",
    "elif dataMode==\"ValHf\":\n",
    "    #Do high frequency splitting of train/test interleaving\n",
    "    splitTrain = 30\n",
    "    splitValid = 4\n",
    "    allowOverlap = False\n",
    "elif dataMode==\"ValLf\":\n",
    "    #Do low frequency splitting of train/test interleaving\n",
    "    splitTrain = 300\n",
    "    splitValid = 40\n",
    "    allowOverlap = False\n",
    "else:\n",
    "    raise ValueError('Unknown data mode:' + dataMode)\n",
    "\n",
    "dataName = \"T\" + str(T) + dataMode + testMode + fixMode\n",
    "    \n",
    "\n",
    "# From an array of input values, first duplicate the first inputs (T-1) times\n",
    "# Then take runs of T inputs to form rows with the inputs at time t,  plus T-1 previous inputs \n",
    "def expand(ins):\n",
    "    padded = np.concatenate( [ np.tile(ins[0], (T-1,1) ), ins ], axis=0 )\n",
    "    N = padded.shape[0]\n",
    "    return np.concatenate( [padded[o:o+N-T+1] for o in range(T) ], axis=1)\n",
    "\n",
    "# If the output or the zn valid are invalid, the sequence is invalid\n",
    "seqValid = np.copy(outsValid) * znValid\n",
    "    \n",
    "was = seqValid.sum()\n",
    "# The sequence is invalid if any of the previous (T-1) steps were invalid\n",
    "for t in range(T-1):\n",
    "    seqValid[1:] *= seqValid[:-1]\n",
    "    seqValid[0] = 0\n",
    "\n",
    "print(\"Valid samples:\", was,\"->\",seqValid.sum())\n",
    "\n",
    "\n",
    "N = vals_y.shape[0]\n",
    "seqIns = expand(vals_x)\n",
    "seqOuts = vals_y\n",
    "\n",
    "isTraining = np.zeros(N)\n",
    "isTesting = np.zeros(N)\n",
    "train_xlist = []\n",
    "train_ylist = []\n",
    "valid_xlist =[]\n",
    "valid_ylist =[]\n",
    "test_xlist =[]\n",
    "test_ylist =[]\n",
    "\n",
    "# Hold some back for testing?\n",
    "testStart = N  * (2 if trainOnAllData else 0.85)\n",
    "\n",
    "tfill = 0\n",
    "vfill = 0\n",
    "valStart = -100\n",
    "prevTrain = -1\n",
    "\n",
    "# Place values in train/test/valid\n",
    "# Don't take random samples, but instead ensure that the train and validation samples do not overlap in time\n",
    "for idx in range(N):\n",
    "    if seqValid[idx]:\n",
    "        if idx>testStart:\n",
    "            test_xlist.append(seqIns[idx])\n",
    "            test_ylist.append(seqOuts[idx])\n",
    "        elif tfill<splitTrain:\n",
    "            if prevTrain<0:\n",
    "                prevTrain = idx\n",
    "            # take 2 values for use in augmentation\n",
    "            train_xlist.append( (seqIns[idx],seqIns[prevTrain]) )\n",
    "            train_ylist.append( (seqOuts[idx],seqOuts[prevTrain]) )\n",
    "            prevTrain = idx\n",
    "            isTraining[idx] = 1\n",
    "            if not allowOverlap:\n",
    "                valStart = idx+T\n",
    "            tfill+=1\n",
    "        elif idx>=valStart:\n",
    "            prevTrain = -1\n",
    "            isTesting[idx] = 1\n",
    "            valid_xlist.append(seqIns[idx])\n",
    "            valid_ylist.append(seqOuts[idx])\n",
    "            vfill+=1\n",
    "            if vfill>splitValid:\n",
    "                vfill = tfill = 0\n",
    "    else:\n",
    "        prevTrain = -1\n",
    "  \n",
    "print(\"Training:\",len(train_xlist), \"Validation:\", len(valid_xlist), \"Test:\", len(test_xlist))\n",
    "train_x = np.array(train_xlist, dtype=np.float)\n",
    "train_y = np.array(train_ylist, dtype=np.float)\n",
    "valid_x = np.array(valid_xlist, dtype=np.float)\n",
    "valid_y = np.array(valid_ylist, dtype=np.float)\n",
    "test_x = np.array(test_xlist, dtype=np.float)\n",
    "test_y = np.array(test_ylist, dtype=np.float)\n",
    "    \n",
    "\n",
    "\n",
    "v0 = 500\n",
    "v1 = 1000\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.gca().set_title(dataName)\n",
    "#plt.plot(outsValid[v0:v1]*znValid[v0:v1])\n",
    "#plt.plot(seconds[v0:v1])\n",
    "plt.plot(isTraining[v0:v1])\n",
    "plt.plot(isTesting[v0:v1])\n",
    "plt.plot(vals_y[v0:v1,0] * (1 - (1-isTraining[v0:v1]) * (1-isTesting[v0:v1])) )\n",
    "plt.plot(vals_y[v0:v1,1] * (1 - (1-isTraining[v0:v1]) * (1-isTesting[v0:v1])) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel2(ins,outs,dropout=0.0,loss='mse',fdo=0.0):\n",
    "    i = Input(shape=(ins,),name=\"input\" )\n",
    "    x = i\n",
    "    dim = 512\n",
    "    x = Dense(dim,activation='relu',name=\"expand\")(x)\n",
    "    x0 = x\n",
    "    for scale in range(1,3):\n",
    "        name = \"d\" + str(scale)\n",
    "        x0 = x\n",
    "        x = Dense(dim,activation='linear', name=name+\"1\")(x)\n",
    "        x = BatchNormalization(axis=-1,name=name + \"bn1\" )(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        if (dropout>0):\n",
    "            d=Dropout(dropout)(x)\n",
    "        x = Dense(dim,activation='linear', name=name+\"2\")(x) \n",
    "        x = BatchNormalization(axis=-1,name=name+\"bn2\")(x)\n",
    "\n",
    "        x = Add(name=\"add\" + str(scale))([x0,x])\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    auxOut = Dense(outs-2,activation='linear',name=\"aux\" )(x)\n",
    "    \n",
    "    \n",
    "    state = Dense(dim-ins-outs+2,activation='relu',name=\"state\")(x)\n",
    "    x = Concatenate()([i,auxOut,state])\n",
    "    \n",
    "    for scale in range(3,5):\n",
    "        name = \"d\" + str(scale)\n",
    "        x0 = x\n",
    "        x = Dense(dim,activation='linear', name=name+\"1\")(x)\n",
    "        x = BatchNormalization(axis=-1,name=name + \"bn1\" )(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        if (dropout>0):\n",
    "            d=Dropout(dropout)(x)\n",
    "        x = Dense(dim,activation='linear', name=name+\"2\")(x) \n",
    "        x = BatchNormalization(axis=-1,name=name+\"bn2\")(x)\n",
    "\n",
    "        x = Add(name=\"add\" + str(scale))([x0,x])\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "    \n",
    "    x = Dense(768,activation='relu',name=\"preout\")(x)\n",
    "    if fdo>0:\n",
    "        x = Dropout(fdo)(x)\n",
    "    x = Dense(2,activation='linear',name=\"recovery\" )(x)\n",
    "        \n",
    "    model = Model(inputs=i, outputs=[x,auxOut] )\n",
    "    model.compile(loss=loss, optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#def runModel(model,ins):\n",
    "#    vals_xt = expand((ins-inMean)/inStd)\n",
    "#    return np.clip(model.predict(vals_xt)*outStd + outMean,0,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srcGenerator2(vals_x, vals_y0, vals_y1, batch_size, augMode):\n",
    "    n = vals_x.shape[0]\n",
    "    shuffle_indices = np.arange(n)\n",
    "    xsize = vals_x.shape[2]\n",
    "    y0size = vals_y0.shape[2]\n",
    "    y1size = vals_y1.shape[2]\n",
    "\n",
    "    while True:\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            count = end-start\n",
    "            \n",
    "            src_batch = np.empty( (count,xsize), dtype=np.float32)\n",
    "            target0_batch = np.empty( (count,y0size), dtype=np.float32)\n",
    "            target1_batch = np.empty( (count,y1size), dtype=np.float32)\n",
    "\n",
    "            for i in range(count):\n",
    "                sid = shuffle_indices[start+i]\n",
    "                # augmentation by mixing linearly between samples\n",
    "                if augMode=='Lin':\n",
    "                    blend = np.random.random()\n",
    "                elif augMode=='Cos':\n",
    "                    blend = np.cos( np.random.random()*math.pi )*0.5 + 0.5\n",
    "                elif augMode=='Sqr':\n",
    "                    blend = (1-np.random.random() * np.random.random())*0.5\n",
    "                elif augMode=='NoAug':\n",
    "                    blend = 0\n",
    "                else:\n",
    "                    raise ValueError(\"Bad augMode:\" + augMode)\n",
    "                \n",
    "                src_batch[i] = vals_x[sid,0] + (vals_x[sid,1] - vals_x[sid,0]) * blend\n",
    "                target0_batch[i] = vals_y0[sid,0]  + (vals_y0[sid,1] - vals_y0[sid,0]) * blend\n",
    "                target1_batch[i] = vals_y1[sid,0]  + (vals_y1[sid,1] - vals_y1[sid,0]) * blend                \n",
    "                \n",
    "            yield src_batch, [target0_batch, target1_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(m,x):\n",
    "    return np.clip( m.predict(x)[0]*allOutStd[0:2] + allOutMean[0:2],0, 100)\n",
    "\n",
    "def runModelRaw(m,ins):\n",
    "    vals_xt = expand((ins-inMean)/inStd)\n",
    "    return runModel(m,vals_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_outs0 = train_y[:,:,0:2]\n",
    "final_outs1 = train_y[:,:,2:]\n",
    "if valid_y.shape[0]>0:\n",
    "    final_valid0 = valid_y[:,0:2]\n",
    "    final_valid1 = valid_y[:,2:]\n",
    "if test_y.shape[0]>0:\n",
    "    final_test0 = test_y[:,0:2]\n",
    "    final_test1 = test_y[:,2:]\n",
    "    \n",
    "\n",
    "print(\"Final outs:\", final_outs0.shape,final_outs1.shape, )\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "DropOut = 0.2\n",
    "FinalDropOut = 0.3\n",
    "augMode = \"Lin\"\n",
    "#augMode = \"NoAug\"\n",
    "#augMode = \"Cos\"  # Beta?\n",
    "#augMode = \"Sqr\"\n",
    "\n",
    "trainName = dataName + \"Bs\" + str(BATCH_SIZE) + \"Do\" + str( (int)(DropOut*100)) + \"FDo\" + str( (int)(FinalDropOut*100)) + augMode\n",
    "\n",
    "def schedule(e):\n",
    "    if e<25:\n",
    "        return 0.001000\n",
    "    if e<40:\n",
    "        return 0.000250\n",
    "    if e<45:\n",
    "        return 0.000050\n",
    "    return     0.000010\n",
    "\n",
    "\n",
    "if valid_y.shape[0]==0:\n",
    "    callbacks = [\n",
    "        LearningRateScheduler(schedule, verbose=0)\n",
    "    ]\n",
    "else:\n",
    "    lossName = \"val_recovery_loss\"\n",
    "    callbacks = [EarlyStopping(monitor=lossName, patience=7,verbose=1,min_delta=1e-4,mode='min'),\n",
    "             ReduceLROnPlateau(monitor=lossName, factor=0.2,patience=5,verbose=1,min_delta=1e-4,  mode='min'),\n",
    "            ]\n",
    "\n",
    "    \n",
    "\n",
    "trainBatches = int( math.ceil( train_x.shape[0]/ (32) ) )\n",
    "\n",
    "refined = createModel2(train_x.shape[-1],train_y.shape[-1],DropOut,mean_absolute_error,FinalDropOut)\n",
    "\n",
    "if valid_y.shape[0]>0:\n",
    "    validation_data=(valid_x, [final_valid0, final_valid1 ])\n",
    "else:\n",
    "    validation_data=(test_x, [final_test0, final_test1 ]) if test_y.shape[0]>0 else ( )\n",
    "    \n",
    "print(\"Training\", dataName, \"->\", trainName)\n",
    "\n",
    "hist = refined.fit_generator( srcGenerator2(train_x, final_outs0, final_outs1,BATCH_SIZE,augMode ),\n",
    "              steps_per_epoch=trainBatches,\n",
    "              epochs=30 if trainOnAllData else 100,\n",
    "              verbose=2,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=validation_data\n",
    "            )\n",
    "\n",
    "\n",
    "plt.plot(hist.history[\"recovery_loss\"])\n",
    "if valid_y.shape[0]>0:\n",
    "    plt.plot(hist.history[lossName])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run\n",
    "testR = runModelRaw(refined,test.values[:,1:])\n",
    "#Glue on the dates\n",
    "testFrame = pd.DataFrame(testR,columns=['rougher.output.recovery','final.output.recovery' ])\n",
    "result = pd.concat([testDate,testFrame],axis=1)\n",
    "#Save\n",
    "\n",
    "outputFile = \"../submissions/\" + trainName + \".csv\"\n",
    "\n",
    "if os.path.isfile(outputFile):\n",
    "    display(Markdown(\"# FILE EXISTS! - not overwriting\"))\n",
    "else:\n",
    "    result.to_csv(outputFile,index=False, sep=',', encoding='utf-8')\n",
    "    print(\"Saved\",outputFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not trainOnAllData):\n",
    "    pred = runModel(refined,test_x)\n",
    "    ref = (test_y*allOutStd+allOutMean)[:,0:2]\n",
    "\n",
    "    print(\"Test Err\", np.sqrt( np.square(pred-ref).mean())  )\n",
    "    print(\"Test Abs\", np.abs(pred-ref).mean() )\n",
    "    val_loss = hist.history[\"val_recovery_loss\"]\n",
    "\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(ref[:,0])\n",
    "    plt.plot(pred[:,0])\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(ref[:,1])\n",
    "    plt.plot(pred[:,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to official entry\n",
    "\n",
    "if os.path.isfile(compareFile):\n",
    "    ref = pd.read_csv(compareFile)\n",
    "    refValid = ref.notna().all(axis=1).values\n",
    "    ref = ref.values[:,1:]\n",
    "    N = ref.shape[0]>>1\n",
    "    \n",
    "\n",
    "    publicRef = ref[:N][refValid[:N]]\n",
    "    publicTest = testR[:N][refValid[:N]]\n",
    "    diff = publicTest-publicRef\n",
    "    \n",
    "    # First half of data assumed to be the public leaderboard month\n",
    "    print(\"Public Test Err\",  np.sqrt( np.square(diff).mean() ) )\n",
    "    print(\"Public Test Abs\",  np.abs(diff).mean() )\n",
    "    print()\n",
    "\n",
    "    privateRef = ref[N:][refValid[N:]]\n",
    "    privateTest = testR[N:][refValid[N:]]\n",
    "    diff = privateTest-privateRef\n",
    "\n",
    "    # Seconf half of data assumed to be the private leaderboard month\n",
    "    print(\"Private Test Err\",  np.sqrt( np.square(diff).mean() ) )\n",
    "    print(\"Private Test Abs\",  np.abs(diff).mean() )\n",
    "    print()\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(testR[:,0])\n",
    "    plt.plot(ref[:,0])\n",
    "    plt.plot(refValid*100,alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(testR[:,1])\n",
    "    plt.plot(ref[:,1])\n",
    "    plt.plot(refValid*100,alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
